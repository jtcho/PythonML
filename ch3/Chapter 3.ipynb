{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: A Tour of Machine Learning Classifieres Using Scikit-learn\n",
    "\n",
    "**Key Steps in Training a ML Algorithm**\n",
    "1. Selection of features.\n",
    "2. Choosing a performance metric.\n",
    "3. Choosing a classifier and optimization algorithm.\n",
    "4. Evaluating the performance of the model.\n",
    "5. Tuning the algorithm.\n",
    "\n",
    "<script type=\"text/javascript\" async\n",
    "  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "# Estimate the sample mean and standard deviation on the training data.\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified samples: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    # Set up marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # Plot the decision surface\n",
    "    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    x2_min, x2_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    # Generate a grid over the field R2 - with the given resolution.\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    # Note that np.array([xx1.ravel(), xx2.ravel()]).T gives permutations of coordinates\n",
    "    # over the grid.\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # Plot the class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                   alpha=0.8, c=cmap(idx),\n",
    "                   marker=markers[idx], label=cl)\n",
    "    \n",
    "    # Highlight test samples\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:,0], X_test[:, 1], c='',\n",
    "                   alpha=1.0, linewidths=1, marker='o',\n",
    "                   s=55, label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X=X_combined_std,\n",
    "                      y=y_combined,\n",
    "                      classifier=ppn,\n",
    "                      test_idx=range(105, 150))\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We note that the perceptron will never converge on datasets that are not linearly separable!\n",
    "\n",
    "Logistic regression is a simple yet poweful algorithm for linear and binary classification problems (somewhat of a misnomer, as it is not actually used for regression.)\n",
    "\n",
    "**odds ratio**: odds in favor of a particular event, $\\frac{p}{1-p}$\n",
    "where $p$ is the probability of the positive event (the one we want to predict)\n",
    "\n",
    "**logit** function: $$logit(p) = \\log \\frac{p}{1 - p}$$\n",
    "\n",
    "We use the logit function to express a linear relationship between feature values and the log-odds.\n",
    "\n",
    "$$logit\\big(p\\big(y=1 \\mid \\mathbf{x}\\big)\\big) = w_0 x_0 + w_1 x_1 + \\cdots + w_m x_m = \\sum_{i=0}^m w_i x_i = \\mathbf{w}^T \\mathbf{x}$$\n",
    "\n",
    "What we really want is the probability that a certain sample belongs to a particular class, i.e. the inverse form of the logit function, the **logistic** function:\n",
    "$$\\phi(z) = \\frac{1}{1+ e^{-z}}$$\n",
    "\n",
    "where $z$ is the net input (linear comb. of weights/features), $z = \\mathbf{w}^T \\mathbf{x}$.\n",
    "\n",
    "So why use the logistic function? Note that the logit function maps values from the range $[0,1]$ to the reals. Then, the logistic function as the inverse maps the reals to the range $[0,1]$, which may be interpreted as probability. \n",
    "\n",
    "Additionally, note that $\\phi(z) = 0.5 \\implies z = 0$.\n",
    "\n",
    "Logistic regression entails replacing the activation function from before with the sigmoid function.\n",
    "\n",
    "The output of the sigmoid function can then be interpreted as the probability of the particular sample belonging to class $1$ $\\phi(z) = P(y = 1\\mid \\mathbf{x};\\mathbf{w})$.\n",
    "\n",
    "For binary classification, the complement gives the probability of the sample belonging to class $0$.\n",
    "\n",
    "A quantizer function (e.g. some step function) can then be used to convert the probabilities into a binary outcome.\n",
    "\n",
    "It *is* often helpful to be able to estimate class-membership probability in addition to simply outputting a single yes/no result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
